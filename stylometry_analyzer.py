# -*- coding: utf-8 -*-
"""Untitled30.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p6mUZk6loS8tveyVDOilzuPZbDCArYVq
"""

# -*- coding: utf-8 -*-
"""
Toolkit di Analisi Stilometrica e Complessità Testuale.

Questo script carica un file (CSV o Parquet), applica un'analisi NLP avanzata
utilizzando spaCy e calcola una vasta gamma di feature stilometriche.

Esecuzione:

1. Installa le dipendenze:
   pip install -r requirements.txt

2. Scarica il modello spaCy:
   python -m spacy download it_core_news_lg

3. Esegui l'analisi:
   python stylometry_analyzer.py \
       --input_file "percorso/ai/tuoi/dati.csv" \
       --output_file "percorso/per/i/risultati.csv" \
       --text_column "testo" \
       --id_column "nome" \
       --run_complexity --run_pos --run_hapax
"""

import spacy
import pandas as pd
import numpy as np
import re
import argparse
import logging
from math import sqrt
from collections import Counter, defaultdict
from tqdm import tqdm
from functools import reduce

# Import ML
from sklearn.preprocessing import normalize
from sklearn.feature_extraction.text import TfidfVectorizer

# --- Configurazione del Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# =============================================================================
# SEZIONE 1: HELPER DI PRE-PROCESSING E UTILITY
# =============================================================================

def text_preprocessing(testo: str) -> str:
    """Applica una pulizia regex estesa per normalizzare gli acronimi legali e il testo."""
    if not isinstance(testo, str):
        return ""
    # (Tutte le tue regole regex da text_preprocessing qui...)
    testo = re.sub("''", "'", testo)
    testo = re.sub(r'www\.', r'www', testo, flags=re.IGNORECASE)
    testo = re.sub(r'\.com', r'com', testo, flags=re.IGNORECASE)
    testo = re.sub(r'\.it', r'it', testo, flags=re.IGNORECASE)
    testo = re.sub(r'nd\.r\.', r'ndr', testo, flags=re.IGNORECASE)
    testo = re.sub(r'n\.d\.r\.', r'ndr', testo, flags=re.IGNORECASE)
    testo = re.sub(r's\. ?r\. ?l\.', r'srl', testo, flags=re.IGNORECASE)
    testo = re.sub(r's\. ?m ?.i\.', r'smi', testo, flags=re.IGNORECASE)
    testo = re.sub(r'c\. ?c\.', r'cc', testo, flags=re.IGNORECASE)
    testo = re.sub(r's\. ?p\. ?a\.', r'spa', testo, flags=re.IGNORECASE)
    testo = re.sub(r'prot\.', r'prot', testo, flags=re.IGNORECASE)
    testo = re.sub(r'lett\.', r'lett', testo, flags=re.IGNORECASE)
    testo = re.sub(r'art\.', r'art', testo, flags=re.IGNORECASE)
    testo = re.sub(r'par\.', r'par', testo, flags=re.IGNORECASE)
    testo = re.sub(r'sig\.', r'sig', testo, flags=re.IGNORECASE)
    testo = re.sub(r'ss\.', r'ss', testo, flags=re.IGNORECASE)
    testo = re.sub(r'd\. ?l\.', r'dl', testo, flags=re.IGNORECASE)
    testo = re.sub(r'd\. ?lgs\.', r'dlgs', testo, flags=re.IGNORECASE)
    testo = re.sub(r'v\.', r'v', testo, flags=re.IGNORECASE)
    testo = re.sub(r'cfr\.', r'cfr', testo, flags=re.IGNORECASE)
    testo = re.sub(r'vd\.', r'vd', testo, flags=re.IGNORECASE)
    testo = re.sub(r'd\. ?p\. ?r.', r'dpr', testo, flags=re.IGNORECASE)
    testo = re.sub(r'ter\.', r'ter', testo, flags=re.IGNORECASE)
    testo = re.sub(r'p\. ?a\.', r'pa', testo, flags=re.IGNORECASE)
    testo = re.sub(r'g\. ?u\.', r'gu', testo, flags=re.IGNORECASE)
    testo = re.sub(r'c\. ?p\.', r'cp', testo, flags=re.IGNORECASE)
    testo = re.sub(r'cpc\.', r'cpc', testo, flags=re.IGNORECASE)
    testo = re.sub(r'cpp\.', r'cpp', testo, flags=re.IGNORECASE)
    testo = re.sub(r'c\. ?d\. ?s\.', r'cds', testo, flags=re.IGNORECASE)
    testo = re.sub(r'c\. ?d\. ?c\.', r'cdc', testo, flags=re.IGNORECASE)
    testo = re.sub(r'c\. ?g\. ?a\.', r'cga', testo, flags=re.IGNORECASE)
    testo = re.sub(r'c\. ?n\. ?f\.', r'cnf', testo, flags=re.IGNORECASE)
    testo = re.sub(r'co\.', r'co', testo, flags=re.IGNORECASE)
    testo = re.sub(r'lett\.', r'lett', testo, flags=re.IGNORECASE)
    testo = re.sub(r'l\.', r'l', testo, flags=re.IGNORECASE)
    testo = re.sub(r'cir\.', r'cir', testo, flags=re.IGNORECASE)
    testo = re.sub(r'circ\.', r'circ', testo, flags=re.IGNORECASE)
    testo = re.sub(r'del\.', r'del', testo, flags=re.IGNORECASE)
    testo = re.sub(r'o\. ?m\.', r'om', testo, flags=re.IGNORECASE)
    testo = re.sub(r'reg\.', r'reg', testo, flags=re.IGNORECASE)
    testo = re.sub(r'all\.', r'all', testo, flags=re.IGNORECASE)
    testo = re.sub(r'c\. ?d\.', r'cd', testo, flags=re.IGNORECASE)
    testo = re.sub(r't\. ?u\. ?a.', r'tua', testo, flags=re.IGNORECASE)
    testo = re.sub(r'cd\.', r'cd', testo, flags=re.IGNORECASE)
    testo = re.sub(r'sez\.', r'sez', testo, flags=re.IGNORECASE)
    testo = re.sub(r'cass\.', r'cass', testo, flags=re.IGNORECASE)
    testo = re.sub(r'civ\.', r'civ', testo, flags=re.IGNORECASE)
    testo = re.sub(r'd\. ?m\.', r'dm', testo, flags=re.IGNORECASE)
    testo = re.sub(r'd ?m\.', r'dm', testo, flags=re.IGNORECASE)
    testo = re.sub(r'n\.', r'n', testo, flags=re.IGNORECASE)
    testo = re.sub(r'c\.', r'c', testo, flags=re.IGNORECASE)
    testo = re.sub(r'\u00a0', r' ', testo)
    testo = re.sub(r'\ufeff', r' ', testo)
    testo = re.sub(r'\u200b', r' ', testo)
    testo = re.sub(r'\x0C', r' ', testo)
    testo = re.sub(r'\xa0', r' ', testo)
    testo = re.sub(r'\s+', ' ', testo)
    return testo.strip()

def normalizza_l2(df: pd.DataFrame, id_column: str) -> pd.DataFrame:
    """Applica la normalizzazione L2 a tutte le colonne tranne la colonna ID."""
    if id_column not in df.columns:
        logging.warning(f"Colonna ID '{id_column}' non trovata per la normalizzazione L2.")
        return df

    ids = df[[id_column]].copy()
    dati = df.drop(columns=[id_column])
    dati_normalizzati = normalize(dati, norm='l2', axis=1)
    df_normalizzato = pd.DataFrame(dati_normalizzati, columns=dati.columns)

    df_normalizzato.reset_index(drop=True, inplace=True)
    ids.reset_index(drop=True, inplace=True)

    df_normalizzato.insert(0, id_column, ids)
    return df_normalizzato

# =============================================================================
# SEZIONE 2: FUNZIONI DI ESTRAZIONE FEATURE (per-Doc)
# Queste funzioni prendono un singolo 'spacy.Doc' e restituiscono un valore
# o un dizionario di valori.
# =============================================================================

def get_valid_sentences(doc: spacy.tokens.Doc) -> list:
    """Restituisce una lista di frasi con più di 3 token validi."""
    valid_sentences = []
    if not hasattr(doc, "sents"):
        return []
    for sent in doc.sents:
        valid_tokens = [token for token in sent if not token.is_punct and not token.is_space]
        if len(valid_tokens) > 3:
            valid_sentences.append(sent)
    return valid_sentences

def corrected_type_token_ratio(doc: spacy.tokens.Doc) -> float:
    """Calcola la CTTR (types / √(2 * tokens)) escludendo punteggiatura e stop word."""
    if doc is None or len(doc) == 0:
        return 0.0
    tokens = [token for token in doc if not token.is_punct and not token.is_space and not token.is_stop]
    if not tokens:
        return 0.0
    types = set(token.lemma_.lower() for token in tokens)
    return round(len(types) / sqrt(2 * len(tokens)), 2)

def avg_sentence_length(doc: spacy.tokens.Doc) -> float:
    """Calcola la lunghezza media delle frasi (solo frasi > 3 parole)."""
    valid_sentences = get_valid_sentences(doc)
    if not valid_sentences:
        return 0.0
    words_count = sum(len([token for token in sent if not token.is_punct and not token.is_space]) for sent in valid_sentences)
    return round(words_count / len(valid_sentences), 2)

def compute_depth(node) -> int:
    """Calcola ricorsivamente la profondità dell'albero sintattico."""
    if node is None or not hasattr(node, "children"):
        return 0
    children = list(node.children)
    if not children:
        return 0
    return 1 + max(compute_depth(child) for child in children)

def count_nodes_per_level(node, level=0, levels=None) -> dict:
    """Conta i nodi per livello nell'albero sintattico."""
    if levels is None:
        levels = defaultdict(int)
    if node is None or not hasattr(node, "children"):
        return levels
    levels[level] += 1
    for child in node.children:
        count_nodes_per_level(child, level + 1, levels)
    return levels

def calculate_avg_depth_width(doc: spacy.tokens.Doc) -> dict:
    """Calcola profondità e larghezza media delle frasi (solo frasi > 3 parole)."""
    all_depths = []
    all_widths = []
    valid_sentences = get_valid_sentences(doc)

    for sent in valid_sentences:
        roots = [token for token in sent if token.head == token]
        if not roots:
            continue

        root = roots[0]
        all_depths.append(compute_depth(root))

        levels = count_nodes_per_level(root)
        if levels:
            all_widths.append(max(levels.values()))

    avg_depth = round(sum(all_depths) / len(all_depths), 2) if all_depths else 0.0
    avg_width = round(sum(all_widths) / len(all_widths), 2) if all_widths else 0.0

    return {"avg_depth": avg_depth, "avg_width": avg_width}

def avg_subordinates_per_sentence(doc: spacy.tokens.Doc) -> float:
    """Calcola la media di frasi subordinate per frase."""
    if doc is None or not hasattr(doc, "sents"):
        return 0.0

    total_subordinates = 0
    total_sentences = 0
    for sent in doc.sents:
        if not hasattr(sent, "__iter__"):
            continue
        total_sentences += 1
        for token in sent:
            if hasattr(token, "dep_") and token.dep_ in {"advcl", "ccomp", "xcomp", "acl:relcl", "acl"}:
                total_subordinates += 1

    return round(total_subordinates / total_sentences, 2) if total_sentences > 0 else 0.0

def avg_passive_verbs(doc: spacy.tokens.Doc) -> float:
    """Calcola la media di verbi passivi per frase."""
    if doc is None or not hasattr(doc, "sents"):
        return 0.0

    passive_verb_count = 0
    sentence_count = 0
    for sent in doc.sents:
        if not hasattr(sent, "__iter__"):
            continue
        sentence_count += 1
        for token in sent:
            if not (hasattr(token, "pos_") and hasattr(token, "dep_") and hasattr(token, "children")):
                continue
            if token.pos_ == "VERB":
                is_passive = token.dep_ == "aux:pass" or any(child.dep_ == "aux:pass" for child in token.children)
                if is_passive:
                    passive_verb_count += 1

    return round(passive_verb_count / sentence_count, 2) if sentence_count > 0 else 0.0

# =============================================================================
# SEZIONE 3: MODULI DI ANALISI (per-DataFrame)
# Queste funzioni prendono la lista di 'spacy.Doc' e restituiscono
# un DataFrame di feature.
# =============================================================================

def run_complexity_analysis(docs: list, ids: list) -> pd.DataFrame:
    """
    Esegue la suite di analisi di complessità su una lista di documenti spaCy.
    Restituisce un DataFrame con una riga per documento.
    (Questa è la tua 'calculate_complexity_features' rifattorizzata)
    """
    logging.info("Esecuzione del modulo di analisi della complessità...")
    results = []
    for doc in tqdm(docs, desc="Analisi Complessità"):
        if doc is None:
            # Aggiungi un record vuoto per mantenere l'allineamento
            results.append({
                "cttr": 0.0,
                "avg_sent_len": 0.0,
                "avg_depth": 0.0,
                "avg_width": 0.0,
                "avg_subordinates": 0.0,
                "avg_passives": 0.0
            })
            continue

        depth_width = calculate_avg_depth_width(doc)

        results.append({
            "cttr": corrected_type_token_ratio(doc),
            "avg_sent_len": avg_sentence_length(doc),
            "avg_depth": depth_width["avg_depth"],
            "avg_width": depth_width["avg_width"],
            "avg_subordinates": avg_subordinates_per_sentence(doc),
            "avg_passives": avg_passive_verbs(doc)
        })

    df = pd.DataFrame(results)
    df.insert(0, 'id', ids)
    return df

def run_pos_analysis(docs: list, ids: list) -> pd.DataFrame:
    """
    Estrae le frequenze normalizzate dei Part-of-Speech (POS).
    (Rifattorizzazione di 'estrai_pos_normalizzati')
    """
    logging.info("Esecuzione del modulo di analisi POS...")
    tutte_le_pos = set()
    documenti_pos = []

    # 1. Prima passata: raccogli tutte le POS e conta
    for doc in tqdm(docs, desc="Analisi POS (Pass 1)"):
        if doc is None:
            documenti_pos.append(Counter())
            continue

        pos = [token.pos_ for token in doc if token.pos_ != "PUNCT"]
        tutte_le_pos.update(pos)
        documenti_pos.append(Counter(pos))

    tutte_le_pos_ordinate = sorted(tutte_le_pos)
    righe = []

    # 2. Seconda passata: calcola frequenze normalizzate
    for contatore in tqdm(documenti_pos, desc="Analisi POS (Pass 2)"):
        lunghezza = sum(contatore.values()) if contatore else 1
        riga = [contatore.get(p, 0) / lunghezza for p in tutte_le_pos_ordinate]
        righe.append(riga)

    df = pd.DataFrame(righe, columns=tutte_le_pos_ordinate)
    df.insert(0, 'id', ids)
    return df

def run_hapax_analysis(docs: list, ids: list) -> pd.DataFrame:
    """
    Estrae gli hapax legomena (lemmi con frequenza 1) per documento.
    (Rifattorizzazione di 'estrai_hapax_su_lemmi')
    """
    logging.info("Esecuzione del modulo di analisi Hapax...")
    lista_hapax_doc = []
    vocabolario_globale = set()

    for doc in tqdm(docs, desc="Analisi Hapax"):
        if doc is None:
            lista_hapax_doc.append({})
            continue

        lemmi = [
            token.lemma_.lower()
            for token in doc
            if token.is_alpha and not token.is_stop
        ]
        contatore = Counter(lemmi)
        hapax = {lemma: 1 for lemma, freq in contatore.items() if freq == 1}
        vocabolario_globale.update(hapax.keys())
        lista_hapax_doc.append(hapax)

    vocab_ordinato = sorted(vocabolario_globale)
    matrice = []
    for hapax_dict in lista_hapax_doc:
        riga = [hapax_dict.get(lemma, 0) for lemma in vocab_ordinato]
        matrice.append(riga)

    df = pd.DataFrame(matrice, columns=vocab_ordinato)
    df.insert(0, 'id', ids)
    return df

def run_pos_trigrams_analysis(docs: list, ids: list) -> pd.DataFrame:
    """
    Estrae trigrammi di POS normalizzati.
    (Rifattorizzazione di 'estrai_trigrammi_pos')
    """
    logging.info("Esecuzione del modulo di analisi Trigrammi POS...")
    tutti_trigrammi = set()
    documenti_trigrammi = []

    for doc in tqdm(docs, desc="Trigrammi POS (Pass 1)"):
        if doc is None:
            documenti_trigrammi.append([])
            continue

        pos_tags = [token.pos_ for token in doc if token.pos_ != "PUNCT"]
        trigrammi = [tuple(pos_tags[i:i+3]) for i in range(len(pos_tags)-2)]
        documenti_trigrammi.append(trigrammi)
        tutti_trigrammi.update(trigrammi)

    trigrammi_ordinati = sorted(tutti_trigrammi)
    # Rinomina le colonne per essere nomi validi (es. ('NOUN', 'ADP', 'DET') -> 'NOUN_ADP_DET')
    colonne_trigrammi = ["_".join(t) for t in trigrammi_ordinati]

    righe = []
    for trigrammi in tqdm(documenti_trigrammi, desc="Trigrammi POS (Pass 2)"):
        contatore = Counter(trigrammi)
        totale = sum(contatore.values()) if contatore else 1
        riga = [contatore.get(t, 0) / totale for t in trigrammi_ordinati]
        righe.append(riga)

    df = pd.DataFrame(righe, columns=colonne_trigrammi)
    df.insert(0, 'id', ids)
    return df

def run_char_trigrams_analysis(texts: list, ids: list) -> pd.DataFrame:
    """
    Estrae trigrammi di caratteri (solo lettere) e i loro conteggi.
    (Rifattorizzazione di 'estrai_trigrammi_caratteri')
    """
    logging.info("Esecuzione del modulo di analisi Trigrammi Caratteri...")
    righe = []
    trigrammi_totali = set()

    for testo in tqdm(texts, desc="Trigrammi Caratteri"):
        if not isinstance(testo, str):
            righe.append({})
            continue

        testo_filtrato = re.sub(r'[^a-zA-ZÀ-ÿ]', '', testo)
        trigrammi = [testo_filtrato[i:i+3] for i in range(len(testo_filtrato)-2)]
        counter = Counter(trigrammi)
        trigrammi_totali.update(counter.keys())

        # Aggiunge il contatore alla lista
        righe.append(counter)

    # Crea DataFrame e riempi i NaN con 0
    df_trigrammi = pd.DataFrame(righe).fillna(0)

    # Ordina le colonne per consistenza
    colonne_trigrammi = sorted([col for col in df_trigrammi.columns])
    df_trigrammi = df_trigrammi[colonne_trigrammi]

    df_trigrammi.insert(0, 'id', ids)
    return df_trigrammi

def run_function_words_analysis(docs: list, ids: list) -> pd.DataFrame:
    """
    Estrae la frequenza normalizzata di parole funzione.
    (Rifattorizzazione di 'estrai_parole_funzione')
    """
    logging.info("Esecuzione del modulo di analisi Parole Funzione...")
    tutti_funzionali = set()
    documenti_funzionali = []

    for doc in tqdm(docs, desc="Parole Funzione (Pass 1)"):
        if doc is None:
            documenti_funzionali.append(([], 1))
            continue

        parole_funzione = [
            token.text.lower()
            for token in doc
            if token.pos_ in {"DET", "ADP", "CCONJ", "SCONJ"}
        ]
        num_token_validi = sum(1 for token in doc if token.is_alpha)
        documenti_funzionali.append((parole_funzione, num_token_validi))
        tutti_funzionali.update(parole_funzione)

    parole_ordinate = sorted(tutti_funzionali)
    righe = []

    for parole_funzione, num_token_validi in tqdm(documenti_funzionali, desc="Parole Funzione (Pass 2)"):
        contatore = Counter(parole_funzione)
        denom = num_token_validi if num_token_validi > 0 else 1
        riga = [contatore.get(p, 0) / denom for p in parole_ordinate]
        righe.append(riga)

    df = pd.DataFrame(righe, columns=parole_ordinate)
    df.insert(0, 'id', ids)
    return df

def run_punctuation_analysis(docs: list, ids: list) -> pd.DataFrame:
    """
    Estrae la frequenza normalizzata della punteggiatura.
    (Rifattorizzazione di 'estrai_punteggiatura_normalizzata')
    """
    logging.info("Esecuzione del modulo di analisi Punteggiatura...")
    punteggiatura_unica = set()
    documenti_punteggiatura = []

    for doc in tqdm(docs, desc="Punteggiatura (Pass 1)"):
        if doc is None:
            documenti_punteggiatura.append((Counter(), 1))
            continue

        punteggiatura = [token.text for token in doc if token.is_punct]
        punteggiatura_unica.update(punteggiatura)
        documenti_punteggiatura.append((Counter(punteggiatura), len(doc)))

    punteggiatura_ordinata = sorted(punteggiatura_unica)
    righe = []

    for contatore, num_token in tqdm(documenti_punteggiatura, desc="Punteggiatura (Pass 2)"):
        denom = num_token if num_token > 0 else 1
        riga = [contatore.get(p, 0) / denom for p in punteggiatura_ordinata]
        righe.append(riga)

    df = pd.DataFrame(righe, columns=punteggiatura_ordinata)
    df.insert(0, 'id', ids)
    return df

def run_tfidf_vocab_analysis(docs: list, ids: list, top_n: int = 500) -> pd.DataFrame:
    """
    Estrae il vocabolario TF-IDF filtrato (lemmi con freq >= 2 in almeno un doc)
    e ridotto ai top_n lemmi più frequenti.
    (Rifattorizzazione di 'estrai_vocabolario_tfidf' e 'seleziona_top_lemmi')
    """
    logging.info("Esecuzione del modulo di analisi TF-IDF Vocabolario...")
    vocabolario_filtrato = set()
    testi_filtrati_per_pass_2 = []

    # 1. Passata 1: costruisci vocabolario filtrato
    for doc in tqdm(docs, desc="TF-IDF Vocab (Pass 1)"):
        if doc is None:
            continue
        lemmi = [
            token.lemma_.lower()
            for token in doc
            if token.is_alpha and not token.is_stop
        ]
        counter = Counter(lemmi)
        lemmi_frequenti = [lemma for lemma, count in counter.items() if count >= 2]
        vocabolario_filtrato.update(lemmi_frequenti)

    # 2. Passata 2: filtra i testi usando il vocabolario
    for doc in tqdm(docs, desc="TF-IDF Vocab (Pass 2)"):
        if doc is None:
            testi_filtrati_per_pass_2.append("")
            continue
        lemmi = [
            token.lemma_.lower()
            for token in doc
            if token.is_alpha and not token.is_stop
               and token.lemma_.lower() in vocabolario_filtrato
        ]
        testi_filtrati_per_pass_2.append(" ".join(lemmi))

    # 3. Calcola TF-IDF e seleziona Top N
    vectorizer = TfidfVectorizer(vocabulary=sorted(vocabolario_filtrato))
    matrice_tfidf = vectorizer.fit_transform(testi_filtrati_per_pass_2)

    df_tfidf = pd.DataFrame(
        matrice_tfidf.toarray(),
        columns=vectorizer.get_feature_names_out()
    )

    # 4. Seleziona i Top N lemmi
    logging.info(f"Selezione dei {top_n} lemmi più frequenti da {df_tfidf.shape[1]}...")
    somma_lemmi = df_tfidf.sum(axis=0)
    top_lemmi = somma_lemmi.nlargest(top_n).index.tolist()
    df_top = df_tfidf[top_lemmi]

    df_top.insert(0, 'id', ids)
    return df_top


# =============================================================================
# SEZIONE 4: ORCHESTRAZIONE E SCRIPT PRINCIPALE
# =============================================================================

def load_data(filepath: str) -> pd.DataFrame:
    """Carica i dati da CSV o Parquet."""
    logging.info(f"Caricamento dati da: {filepath}")
    if filepath.endswith(".csv"):
        return pd.read_csv(filepath)
    elif filepath.endswith(".parquet"):
        return pd.read_parquet(filepath)
    else:
        raise ValueError(f"Formato file non supportato: {filepath}. Usare .csv o .parquet.")

def save_data(df: pd.DataFrame, filepath: str):
    """Salva i dati in CSV o Parquet."""
    logging.info(f"Salvataggio dati in: {filepath}")
    if filepath.endswith(".csv"):
        df.to_csv(filepath, index=False)
    elif filepath.endswith(".parquet"):
        df.to_parquet(filepath, index=False)
    else:
        raise ValueError(f"Formato file non supportato: {filepath}. Usare .csv o .parquet.")

def main(args):
    """
    Funzione principale che orchestra l'intero processo.
    """

    # 1. Carica i dati
    try:
        df = load_data(args.input_file)
    except FileNotFoundError:
        logging.error(f"File di input non trovato: {args.input_file}")
        return
    except Exception as e:
        logging.error(f"Errore nel caricamento dei dati: {e}")
        return

    # Controlla le colonne necessarie
    if args.text_column not in df.columns:
        logging.error(f"Colonna testo '{args.text_column}' non trovata nel file.")
        return
    if args.id_column not in df.columns:
        logging.error(f"Colonna ID '{args.id_column}' non trovata nel file.")
        logging.error("L'analisi richiede una colonna ID univoca (es. 'nome' o 'id').")
        return
    if df[args.id_column].duplicated().any():
        logging.warning(f"ATTENZIONE: La colonna ID '{args.id_column}' contiene duplicati. Il merge potrebbe non funzionare correttamente.")

    # 2. Pre-processa i testi
    logging.info("Applicazione del pre-processing (pulizia regex)...")
    texts_raw = df[args.text_column].fillna("").apply(text_preprocessing).tolist()
    ids = df[args.id_column].tolist()

    # 3. Carica e applica spaCy (UNA SOLA VOLTA)
    logging.info(f"Caricamento modello spaCy '{args.spacy_model}'...")
    try:
        nlp = spacy.load(args.spacy_model)
    except OSError:
        logging.error(f"Modello spaCy '{args.spacy_model}' non trovato.")
        logging.error(f"Esegui: python -m spacy download {args.spacy_model}")
        return

    logging.info(f"Elaborazione di {len(texts_raw)} testi con spaCy... (Questo può richiedere tempo)")
    # Disabilita i pipe non necessari per velocizzare
    # Manteniamo 'parser' per le analisi sintattiche (profondità, subordinate)
    disable_pipes = ["ner"]
    docs = list(tqdm(nlp.pipe(texts_raw, disable=disable_pipes), total=len(texts_raw), desc="spaCy Processing"))

    # Lista dei DataFrame di feature da unire
    features_dfs = [df] # Inizia con il DataFrame originale

    # 4. Esegui i moduli di analisi richiesti

    if args.run_complexity:
        df_complexity = run_complexity_analysis(docs, ids)
        features_dfs.append(df_complexity)

    if args.run_pos:
        df_pos = run_pos_analysis(docs, ids)
        if args.normalize_features:
            logging.info("Normalizzazione L2 per le feature POS...")
            df_pos = normalizza_l2(df_pos, args.id_column)
        features_dfs.append(df_pos)

    if args.run_hapax:
        df_hapax = run_hapax_analysis(docs, ids)
        if args.normalize_features:
            logging.info("Normalizzazione L2 per le feature Hapax...")
            df_hapax = normalizza_l2(df_hapax, args.id_column)
        features_dfs.append(df_hapax)

    if args.run_pos_trigrams:
        df_pos_trigrams = run_pos_trigrams_analysis(docs, ids)
        if args.normalize_features:
            logging.info("Normalizzazione L2 per le feature Trigrammi POS...")
            df_pos_trigrams = normalizza_l2(df_pos_trigrams, args.id_column)
        features_dfs.append(df_pos_trigrams)

    if args.run_char_trigrams:
        # Nota: questo usa 'texts_raw' non 'docs', perché è basato su regex
        df_char_trigrams = run_char_trigrams_analysis(texts_raw, ids)
        if args.normalize_features:
            logging.info("Normalizzazione L2 per le feature Trigrammi Caratteri...")
            df_char_trigrams = normalizza_l2(df_char_trigrams, args.id_column)
        features_dfs.append(df_char_trigrams)

    if args.run_function_words:
        df_func_words = run_function_words_analysis(docs, ids)
        if args.normalize_features:
            logging.info("Normalizzazione L2 per le feature Parole Funzione...")
            df_func_words = normalizza_l2(df_func_words, args.id_column)
        features_dfs.append(df_func_words)

    if args.run_punctuation:
        df_punct = run_punctuation_analysis(docs, ids)
        if args.normalize_features:
            logging.info("Normalizzazione L2 per le feature Punteggiatura...")
            df_punct = normalizza_l2(df_punct, args.id_column)
        features_dfs.append(df_punct)

    if args.run_vocab_tfidf:
        df_tfidf = run_tfidf_vocab_analysis(docs, ids, args.vocab_top_n)
        # TF-IDF è già una misura normalizzata, L2 non è solitamente necessaria
        features_dfs.append(df_tfidf)

    # 5. Unisci tutti i DataFrame di feature
    if len(features_dfs) > 1:
        logging.info("Unione di tutti i DataFrame di feature...")
        try:
            df_finale = reduce(lambda left, right: pd.merge(left, right, on=args.id_column, how='left'), features_dfs)
        except Exception as e:
            logging.error(f"Errore durante il merge dei DataFrame. Assicurati che la colonna ID '{args.id_column}' sia corretta. Errore: {e}")
            return
    else:
        logging.warning("Nessun modulo di analisi è stato selezionato. Il file di output sarà una copia dell'input.")
        df_finale = df

    # 6. Salva i risultati
    try:
        save_data(df_finale, args.output_file)
        logging.info(f"Analisi completata con successo! Risultati salvati in {args.output_file}")
    except Exception as e:
        logging.error(f"Errore nel salvaggio del file di output: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Toolkit di Analisi Stilometrica e Complessità Testuale")

    # Argomenti I/O
    parser.add_argument("-i", "--input_file", type=str, required=True, help="Percorso del file di input (.csv o .parquet)")
    parser.add_argument("-o", "--output_file", type=str, required=True, help="Percorso del file di output (.csv o .parquet)")

    # Argomenti Colonne
    parser.add_argument("--text_column", type=str, default="testo", help="Nome della colonna contenente il testo (default: 'testo')")
    parser.add_argument("--id_column", type=str, default="nome", help="Nome della colonna ID univoca (default: 'nome')")

    # Argomenti Modello
    parser.add_argument("--spacy_model", type=str, default="it_core_news_lg", help="Nome del modello spaCy da scaricare e usare (default: 'it_core_news_lg')")

    # Argomenti per Moduli di Analisi (Flags)
    parser.add_argument("--run_complexity", action="store_true", help="Esegui le metriche di complessità (TTR, profondità, subordinate, etc.)")
    parser.add_argument("--run_pos", action="store_true", help="Esegui l'analisi delle frequenze POS")
    parser.add_argument("--run_hapax", action="store_true", help="Esegui l'analisi degli Hapax Legomena (lemmi)")
    parser.add_argument("--run_pos_trigrams", action="store_true", help="Esegui l'analisi dei trigrammi POS")
    parser.add_argument("--run_char_trigrams", action="store_true", help="Esegui l'analisi dei trigrammi di caratteri")
    parser.add_argument("--run_function_words", action="store_true", help="Esegui l'analisi delle parole funzione")
    parser.add_argument("--run_punctuation", action="store_true", help="Esegui l'analisi della punteggiatura")
    parser.add_argument("--run_vocab_tfidf", action="store_true", help="Esegui l'analisi del vocabolario TF-IDF (filtrato e top-n)")

    # Opzioni di Analisi
    parser.add_argument("--vocab_top_n", type=int, default=500, help="Numero di lemmi top da tenere per l'analisi TF-IDF (default: 500)")
    parser.add_argument("--normalize_features", action="store_true", help="Applica la normalizzazione L2 ai moduli di feature ad alta dimensionalità (POS, Hapax, etc.)")

    args = parser.parse_args()
    main(args)